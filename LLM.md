# Zen Omni - AI Assistant Knowledge Base

**Last Updated**: 2024-12-01
**Project**: zen-omni
**Organization**: zenlm
**Website**: https://zenlm.org

## Project Overview

Zen Omni is a hypermodal language model for translation and audio generation, built on Qwen3-Omni-30B-A3B. It is part of the Zen LM model family by Hanzo AI.

### Key Specifications

| Attribute | Value |
|-----------|-------|
| **Base Model** | Qwen3-Omni-30B-A3B-Instruct |
| **Architecture** | `Qwen3OmniMoeForConditionalGeneration` |
| **Total Parameters** | 30B |
| **Active Parameters** | 3B (via MoE) |
| **Text Languages** | 119 |
| **Speech Input** | 19 languages |
| **Speech Output** | 10 languages |
| **Context Length** | 32,768 tokens |

### Model Variants

| Model | Purpose | HuggingFace |
|-------|---------|-------------|
| zen-omni | General multimodal | zenlm/zen-omni |
| zen-omni-30b-instruct | Instruction following | zenlm/zen-omni-30b-instruct |
| zen-omni-30b-thinking | Extended reasoning | zenlm/zen-omni-30b-thinking |
| zen-omni-30b-captioner | Image/video captioning | zenlm/zen-omni-30b-captioner |

## Repository Structure

```
zen-omni/
â”œâ”€â”€ base-model/              # Downloaded Qwen3-Omni-30B-A3B weights
â”œâ”€â”€ src/zen_omni/            # Python package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py               # Command line interface
â”‚   â”œâ”€â”€ translator.py        # ZenOmniTranslator class
â”‚   â””â”€â”€ pipeline.py          # ZenDubbingPipeline, HanzoOrchestrationLayer
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ zen_identity_sft.yaml     # ms-swift training config
â”‚   â”œâ”€â”€ ds_config_zero2.json      # DeepSpeed config
â”‚   â”œâ”€â”€ train_identity.sh         # Training script
â”‚   â””â”€â”€ data/zen_identity.jsonl   # Identity training data
â”œâ”€â”€ hf-cards/                # HuggingFace model cards
â”‚   â”œâ”€â”€ zen-omni/
â”‚   â”œâ”€â”€ zen-omni-30b-instruct/
â”‚   â”œâ”€â”€ zen-omni-30b-thinking/
â”‚   â””â”€â”€ zen-omni-30b-captioner/
â”œâ”€â”€ paper/
â”‚   â””â”€â”€ zen_omni_technical_report.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ upload_hf_cards.sh
â”œâ”€â”€ docs/                    # Website documentation
â”œâ”€â”€ pyproject.toml           # Python package config
â”œâ”€â”€ README.md                # Main readme
â””â”€â”€ LLM.md                   # This file
```

## Essential Commands

### Development
```bash
# Install package
pip install -e ".[all]"

# Run CLI
zen-omni translate audio.wav --lang en
zen-omni dub video.mp4 --lang en
zen-omni chat
zen-omni caption image.jpg
```

### Training
```bash
# Identity fine-tuning with ms-swift
cd training
./train_identity.sh
```

### Upload to HuggingFace
```bash
# Upload model cards
./scripts/upload_hf_cards.sh

# Upload full model weights
hf upload zenlm/zen-omni ./base-model --repo-type model
```

## Architecture

### Thinker-Talker Architecture

```
INPUT ENCODERS
â”œâ”€â”€ Audio Encoder (32 layers, 1280 dim)
â”œâ”€â”€ Vision Encoder (27 layers, 1152 dim)
â””â”€â”€ Text Embeddings (151,936 vocab)
        â†“
THINKER (Multimodal LLM)
â”œâ”€â”€ 48 transformer layers
â”œâ”€â”€ 128 experts (MoE)
â”œâ”€â”€ 8 experts active per token
â””â”€â”€ Cross-modal attention fusion
        â†“
TALKER (Audio Generator)
â”œâ”€â”€ Code2Wav audio codec
â”œâ”€â”€ 16 quantizers, 2048 codebook
â””â”€â”€ Streaming synthesis (24kHz)
```

## Integration with Zen Dub

Zen Omni integrates with zen-dub for video dubbing:

```python
from zen_omni import ZenDubbingPipeline

pipeline = ZenDubbingPipeline()
pipeline.dub("video.mp4", target_lang="en", output_path="dubbed.mp4")
```

Pipeline stages:
1. Extract audio from video
2. Translate speech with Zen Omni
3. Generate lip-synced video with Zen Dub
4. Composite final output

## Key Technologies

- **Qwen3-Omni**: Base multimodal architecture
- **ms-swift**: ModelScope fine-tuning framework
- **MuseTalk**: Neural lip synchronization (zen-dub)
- **Whisper**: Audio feature extraction
- **DeepSpeed**: Distributed training

## Development Workflow

1. Download base model: `hf download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./base-model`
2. Identity fine-tuning: `./training/train_identity.sh`
3. Test locally: `zen-omni chat`
4. Upload to HuggingFace: `./scripts/upload_hf_cards.sh`

## Context for All AI Assistants

This file (`LLM.md`) is symlinked as:
- `.AGENTS.md`
- `CLAUDE.md`
- `QWEN.md`
- `GEMINI.md`

All files reference the same knowledge base. Updates here propagate to all AI systems.

## Rules for AI Assistants

1. **ALWAYS** update LLM.md with significant discoveries
2. **NEVER** commit symlinked files (.AGENTS.md, CLAUDE.md, etc.) - they're in .gitignore
3. **NEVER** create random summary files - update THIS file
4. Zen models are based on **Qwen3** (NOT Qwen2!)
5. Use `hf` CLI for HuggingFace operations
6. Test-driven development - always verify before marking complete

## Current Status (2024-12-01)

### Completed âœ…
- README.md with correct architecture
- ms-swift training configuration
- Identity training data
- Python package (src/zen_omni/)
- CLI tool
- ZenOmniTranslator class
- ZenDubbingPipeline integration
- HanzoOrchestrationLayer for real-time streaming
- HuggingFace model cards for all variants
- Technical report
- pyproject.toml

### In Progress ðŸ”„
- Downloading Qwen3-Omni-30B-A3B-Instruct weights (~66GB)

### Pending ðŸ“‹
- Identity fine-tuning execution
- Upload fine-tuned weights to HuggingFace
- Integration testing with zen-dub
- Performance benchmarking

---

**Zen Omni**: Hypermodal Language Model for Translation and Audio Generation

**Hanzo AI** | https://hanzo.ai | Techstars '17
**Zoo Labs Foundation** | https://zoolabs.io | 501(c)(3)
