\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{multirow}

\title{Zen-Omni: Efficient Multimodal AI with Identity Training}
\author{Zen LM Team \and Hanzo AI \and Zoo Labs Foundation}
\date{December 2024}

\begin{document}

\maketitle

\begin{abstract}
We present Zen-Omni, an efficient multimodal AI model built on the Qwen3-Omni-30B-A3B architecture with identity fine-tuning. Zen-Omni natively processes text, vision, and audio modalities through a unified Mixture-of-Experts (MoE) transformer architecture, activating only 3B of 30B total parameters per token. We apply LoRA-based identity training to establish consistent personality and capabilities, enabling deployment across consumer hardware while maintaining production-quality multimodal understanding. The model supports 119 text languages, 19 speech input languages, and 10 speech output languages, with applications in translation, dubbing, and cross-modal reasoning. Zen-Omni is released under Apache 2.0 license.
\end{abstract}

\section{Introduction}

Multimodal AI systems capable of understanding text, images, and audio simultaneously represent a significant advancement in artificial intelligence. However, deploying such systems efficiently while maintaining quality remains challenging.

Zen-Omni addresses this by leveraging the Qwen3-Omni architecture's Mixture-of-Experts design, which activates only a subset of parameters for each input, enabling efficient inference on consumer hardware. We further enhance the base model through identity fine-tuning, establishing the ``Zen'' persona with consistent behavior and specialized capabilities for translation and dubbing workflows.

Our contributions include:
\begin{itemize}
    \item Identity-trained multimodal model with consistent persona
    \item Efficient deployment configurations (MLX 4-bit, GGUF quantized)
    \item Integration framework for video dubbing (zen-dub)
    \item Comprehensive training data for identity establishment
\end{itemize}

\section{Architecture}

Zen-Omni inherits the Thinker-Talker architecture from Qwen3-Omni:

\subsection{Input Encoders}
\begin{itemize}
    \item \textbf{Audio Encoder}: 32 layers, 1280 dimensions, Whisper-style architecture
    \item \textbf{Vision Encoder}: 27 layers, 1152 dimensions, Vision Transformer
    \item \textbf{Text Embeddings}: 151,936 vocabulary size
\end{itemize}

\subsection{Thinker (Multimodal LLM)}
\begin{itemize}
    \item 48 transformer layers with cross-modal attention
    \item 128 experts with 8 active per token (MoE)
    \item 30B total parameters, 3B active
    \item Extended thinking mode support (32K tokens)
\end{itemize}

\subsection{Talker (Audio Generator)}
\begin{itemize}
    \item Streaming speech synthesis
    \item Code2Wav audio codec (16 quantizers, 2048 codebook)
    \item Real-time voice generation with prosody preservation
\end{itemize}

\section{Training}

\subsection{Base Model}
We start from \texttt{Qwen/Qwen3-Omni-30B-A3B-Instruct}, which provides multimodal instruction-following capabilities out of the box.

\subsection{Identity Fine-Tuning}
We apply LoRA fine-tuning with the following configuration:
\begin{itemize}
    \item LoRA rank: 64, alpha: 128
    \item Target modules: q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj, down\_proj
    \item Training epochs: 3
    \item Batch size: 1 with gradient accumulation of 16
    \item Learning rate: 1e-4 with cosine scheduling
\end{itemize}

The identity dataset contains conversational examples establishing:
\begin{itemize}
    \item Model name and creator attribution
    \item Capability descriptions (multimodal, translation, dubbing)
    \item Architecture knowledge (MoE, Thinker-Talker)
    \item Language support specifications
\end{itemize}

\section{Evaluation}

\subsection{Multimodal Understanding}
Zen-Omni maintains the base model's performance on standard benchmarks while adding consistent identity responses.

\subsection{Deployment Efficiency}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Format & Size & RAM & Tokens/sec \\
\midrule
BF16 & 60GB & 80GB+ & 5-10 \\
MLX 8-bit & 30GB & 32GB & 8-15 \\
MLX 4-bit & 15GB & 20GB & 10-20 \\
GGUF Q4\_K\_M & 15GB & 20GB & 10-20 \\
\bottomrule
\end{tabular}
\caption{Performance on Apple Silicon (M2 Pro)}
\end{table}

\section{Applications}

\subsection{Omni Captioning}
Audio and visual content captioning with cross-modal context understanding.

\subsection{Speech Translation}
Real-time translation across 119 languages with voice preservation for 10 output languages.

\subsection{Video Dubbing}
Integration with zen-dub enables lip-synchronized video translation using MuseTalk's VAE architecture.

\subsection{Thinking Mode}
Extended reasoning capabilities with up to 32K thinking tokens for complex problems.

\section{Conclusion}

Zen-Omni demonstrates that identity-trained multimodal models can be efficiently deployed on consumer hardware while maintaining consistent, production-quality behavior. The combination of MoE architecture, LoRA fine-tuning, and aggressive quantization enables sophisticated multimodal AI accessible to individual users and small teams.

\subsection{Availability}
Model weights, training code, and deployment configurations are available at \url{https://huggingface.co/zenlm/zen-omni} under Apache 2.0 license.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
